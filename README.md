Newtoki crawling and recommendation
=============

### 목적
한정적인 태그와 검색기능의 한계로 인해 만화를 찾기 어려움을 겪음
확장프로그램이나 앱 개발이 목표
### 문제와 해결방법
* ~~새로운 만화의 업데이트~~ **일주일마다 자동업데이트(8/4)**
* ~~매번 바뀌는 도메인 주소~~ **접속가능한 도메인 주소를 따로 파일로 저장해놓고 업데이트때 읽어서 원활하게 접속(8/5)**
* ~~캡챠~~ **수동 작성**
* IP차단

### 환경
|Python|pip|Selenium|BeautifulSoup4|
|:---:|:---:|:---:|:---:|
|3.11.4|23.2.1|4.11.2|4.12.2|
- - -
> 8월4일
>
#### 1. 파이썬 환경변수 설정과 패키지 설치
* pip install --update pip
* pip install (--update) selenium
* pip install (--update) webdriver_manager

?? 셀레니움 참조가 안됨

#### 2. 파이참 인터프리터 설정
* File > Settings > 톱니바퀴 Add > System Interpreter > 컴퓨터에 설치된 파이썬.exe 경로
<img src="\src\system_interpreter.png"></img>
###### why? 내 컴퓨터에 설치된 패키지들은 파이참의 인터프리터는 인식하지 못하므로 내 컴퓨터에 있는 인터프리터를 사용해야한다.

- - -
> 8월 5일
>
#### 1. github readme 작성
어제 몫까지 작성

#### 2. git 업로드
git 문법을 까먹어서 다시 공부 ./git.md 참고

- - -
> 8월 6일
>
#### 1. 방향성
캡챠, IP차단, 크롤링 후 저장방식 등 여러가지

크롤링 우회
###### https://pythondocs.net/selenium/%EC%85%80%EB%A0%88%EB%8B%88%EC%9B%80-%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%A7%81-%EB%B4%87-%ED%83%90%EC%A7%80-%EC%9A%B0%ED%9A%8C/

* 캡챠 = ~~번거롭지만 wait()을 걸어서 직접 캡챠번호를 입력한다.~~ 사이트 창을 띄우고 로그인을 한 뒤 셀레니움을 돌리면 된다고 한다.
###### https://domdom.tistory.com/235   
###### https://kissi-pro.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%85%80%EB%A0%88%EB%8B%88%EC%9B%80-%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%A1%9C%EA%B7%B8%EC%9D%B8-%EC%9E%90%EB%8F%99%EC%9E%85%EB%A0%A5-%EB%B0%A9%EC%A7%80%EB%AC%B8%EC%9E%90-%EB%9A%AB%EA%B8%B0#google_vignette .... ~~https://gam860720.tistory.com/532 머신러닝은 과하지않나..~~
* IP차단 = 이 경우 언제 차단 당하는지 모르기 때문에 애매하다. 처음부터 vpn을 키고 작업
###### https://domdom.tistory.com/329
++ https 와 vpn 차이 = https는 dns에서 막는 주소를 구글의 DNS를 통해 사이트를 뚫는 거고(ip변경 X) // vpn은 외국 vpn회사의 서버를 통해 사이트를 대신 뚫어준다.
###### https://kbench.com/community/?q=node/16163
* 저장방식 = 일단 엑셀(csv)로 생각하고 있는데 어차피 도메인 주소도 저장해야하므로 나중엔 MySQL로 저장할 예정

#### 2. 크롤링 진행계획
* 10페이지가 안넘어가고 모든 만화를 받을 수 있는 방법을 찾아보니
   1. 요일을 기준으로 진행
  >    
  > 가장 적은 페이지 전환과 아직 페이지의 여유가 있으르모 요일을 기준으로 크롤링하는 것이 좋다고 생각 .. 4471건
  >
   2. ~~초성을 기준으로 진행~~
  >    
  > ~~마찬가지로 한 초성당 10페이지 안으로 있으나 초성들마다 지역성이 있어 추후에는 다른 방법을 찾아야할 가능성이 크다~~

  >!요일기준 검색과 초성기준 검색 결과가 다름 [4471 != 4473] >> 이유 찾는중
  >
  >!요일기준 검색결과 2차 체크 완료 .. *초성기준 검색결과 2차 체크 필요*

* 받아올 정보를 정리해봤다.
  1. 클릭 전
     1. 이미지
     2. 요일
     3. 장르
     4. 제목    
  2. 클릭 후
     1. 별점
     2. 추천
     3. 회차
  3. 그 외
     1. 웹툰 플랫폼
     > '~ 웹툰' 인터넷 검색 > 제일 위에 뜨는거
     > https://blog.zarathu.com/posts/2023-02-15-searchapi-with-python/

- - -
> 8월7일
컨디션 이슈로 인한 휴식

- - -
> 8월 8일
> 구현 시작
* 정리했던 내용들 토대로 크롤링을 진행해봤다.
1. BeautifulSoup4 설치 및 크롤링 시작
   크롤링을 시작해봤다. Selenium은 웹 브라우저를 열고 닫다보니 시간이 오래걸리지만 데이터를 얻기 위해 클릭을 해야하므로 Selenium은 필수불가결했고 페이지에서 데이터를 받아오기에는 BeautifulSoup4가 속도가 빠르므로 두 패키지를 혼합하여 사용하기로 했다.
   !! 생각하니 클릭이벤트가 굳이 필요한가? 요일별로 클릭해놓은 주소를 저장해놓으면 되지 않나?
   !! 들어가도 페이지 넘어가면서 데이터를 받아야하므로 클릭이벤트가 필요할듯하다. == 만약 이 페이지들도 정적으로 저장한다면 추후 웹툰이 추가되어 페이지가 추가될 수 있기 때문이다
   !! 근데 이것도 사실 페이지 갯수를 읽고 저장하면 되지않나? 
   ###### https://deep-flame.tistory.com/27
   
2. ip차단 우회방법
   역시나 크롤링을 하다보니 ip차단이 됐다. 그래서 처음부터 노트북을 핸드폰에서 핫스팟에 연결하고, 차단되면 모바일네트워크를 종료했다 키면 새로운 ip주소를 받아오기 때문에 다시 접속할 수 있게 된다.
   !! 이 방법을 데스크탑 환경에서도 가능하도록 알아봐야겠다.
   !! 크롤링 중간에 차단당하면 멈추기 때문에 지금은 초기단계기 때문에 가능하지만 추후 크롤링할때 차단 안당하도록 알아봐야한다.
   ###### https://daeil.tistory.com/1996
   ###### https://skdrns2.tistory.com/163
   
**!! 문제점**
   1. 일단 가져오고자 하는 데이터가 li태그의 속성값인 것을 알 수 있었지만 이 값을 가져오는 것에 실패했다.
   <img src="\src\뉴토끼_태그속성값데이터.png"></img>
   2. 체력때문에 생각보다 오래 집중할 수 없었다. 빠른 건강회복이 필요할듯 하다.
- - -
> 8월 9일 ~ 8월 10일
>
* 뭉뚱그려서 작성해서 그렇지만 이틀동안 컨디션 회복은 기본으로 크롤링 강의 2편을 완강했다. 최근 비가 와서 집에서 인터넷을 하다보니 잘 잡히지 않아 시간이 오래걸렸다.

- - -
> 8월 11일
>
* 들었던 강의들을 바탕으로 속성값 추출에 성공했다.
* 왜 크롤링이 안됐나 했는데 목적 페이지가 동적 페이지였기 때문에 정적인 url로 바로 들어가면 안됐고, 셀레니움을 통해 동적으로 이동하여 데이터를 받아오도록 했다. >> 이유를 몰라 한참 걸렸고 그 와중에 아이피 차단 당하면서 해서 시간이 더 걸렸다.
* user-agent값을 넣었다고 해도 짧은 시간동안 하는 거라 차단될 가능성이 있어서 현재도 핫스팟으로 진행중 >> 코드를 완성하고 돌렸을 때도 괜찮으면 인터넷망 사용하자
* 데이터 구조를 작성해야겠다고 생각했다 내일은 데이터를 어떻게 받을지 (장르 같은 경우 여러 장르가 
